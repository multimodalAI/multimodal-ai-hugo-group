---
# An instance of the People widget.
# Documentation: https://wowchemy.com/docs/page-builder/
widget: people

# This file represents a page section.
headless: true

# Order that this section appears on the page.
weight: 30

title: Schedule
subtitle: 

content:
  # Choose which groups/teams of users to display.
  #   Edit `user_groups` in each user's profile to add them to one or more of these groups.
#  user_groups:
#    - Principal Investigators
#    - Researchers
#    - Grad Students
#    - Administration
#    - Visitors
#   - Alumni
design:
  show_interests: false
  show_role: true
  show_social: true
---

#### Chair: [Peter Charlton](https://peterhcharlton.github.io/), Co-Chair: [Xianyuan Liu](https://xianyuanliu.github.io/)

<center>

|Time         | &nbsp;&nbsp;&nbsp;&nbsp;Event |
|-------------|-------------------------------|
| 14:30 - 14:35   | &nbsp;&nbsp;&nbsp;&nbsp;**Welcome**   |
| 14:35 - 14:55   | &nbsp;&nbsp;&nbsp;&nbsp;**Opening talk**: _Towards Deployment-Centric Multimodal AI_ -- [Prof. Haiping Lu](https://haipinglu.github.io/), The University of Sheffield  |
| 14:55 - 15:15   | &nbsp;&nbsp;&nbsp;&nbsp;**Introduction to the Tutorials**.    |
| <td style="word-wrap: break-word; max-width:300px; text-align: justify;">&nbsp;&nbsp;&nbsp;&nbsp;Speakers:<ul><li>[Jiayang Zhang](https://linkedin.com/in/jiayang-zhang)</li><li>[Zixuan (Kelly) Ding](https://www.linkedin.com/in/kellydingzx)</li><li>[Xianyuan Liu](https://github.com/xianyuanliu)</li><li>[Sina Tabakhi](https://sinatabakhi.github.io/)</li></td> |
| 15:15 - 15:25   | &nbsp;&nbsp;&nbsp;&nbsp;**Interactive tutorial to the prerequisites** (Colab, Environment Setup, and Configuration)|
| 15:25 - 16:25   | &nbsp;&nbsp;&nbsp;&nbsp;**Hands-On Sessions** (Round 1)  |
| 16:25 - 16:35   | &nbsp;&nbsp;&nbsp;&nbsp;**Open Sharing and Discussion** |
| 16:35 - 17:35   | &nbsp;&nbsp;&nbsp;&nbsp;**Hands-On Sessions** (Round 2)  |
| 17:35 - 17:50   | &nbsp;&nbsp;&nbsp;&nbsp;**Post-tutorial Discussion**             |
| 17:50 - 18:00   | &nbsp;&nbsp;&nbsp;&nbsp;**Closing Remarks** |
|                 | &nbsp;&nbsp;&nbsp;&nbsp;**Networking** |

</center>

<!-- | <td style="word-wrap: break-word; max-width:300px; text-align: justify;"><ul><li>Brain Disorder Diagnosis (Imaging + Phenotypic Features)</li><li> Cardiothoracic Abnormality Assessment (X-ray + ECG)</li><li>Cancer Classification (Multi-omics)</li><li>Drugâ€“Target Interaction Prediction (Protein + Molecular)</li></td>| -->

<!-- These hands-on tutorials will use public imaging, omics, and molecular datasets, including MIMIC ([Chest X-ray](https://physionet.org/content/mimic-cxr/2.1.0/) and [ECG](https://physionet.org/content/mimic-iv-ecg/1.0/)), [ABIDE](https://fcon_1000.projects.nitrc.org/indi/abide/abide_I.html), [TCGA](https://www.cancer.gov/ccg/research/genome-sequencing/tcga), [BindingDB](https://www.bindingdb.org/rwd/bind/index.jsp), and [BioSNAP](https://snap.stanford.edu/biodata/), and follow a standardised machine learning pipeline: data loading, preprocessing, embedding, prediction, evaluation, and interpretation, using the open-source multimodal AI library [PyKale](https://github.com/pykale/pykale). -->
