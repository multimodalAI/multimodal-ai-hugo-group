---
title: "Open Biomedical Multimodal AI Research: From Pixels to Molecules"
subtitle: "EMBC 2025 Workshop · Room B3 M7-8 · 2:30–6:30 PM · 16 July 2025 · Copenhagen, Denmark"
widget: blank
headless: true
weight: 10
design:
  columns: '1'
---

<!-- Are you interested in multimodal AI for healthcare and medicine, and planning to attend EMBC 2025 (14–17 July) in Copenhagen, Denmark?
If so, we invite you to join our half-day workshop on 16 July, *Open Biomedical Multimodal AI Research: From Pixels to Molecules*. Simply select this workshop when [registering](https://embc.embs.org/2025/registration/) for the conference. -->

The rapid expansion of biomedical data from diverse sources presents opportunities to advance healthcare and precision medicine through multimodal AI, which integrates information from multiple data modalities to improve predictive performance and understanding. Open, accessible, and reproducible tools enable the EMBS and wider community to build on state-of-the-art developments efficiently and accelerate innovation.

This workshop aims to equip participants with the skills and tools to address key challenges in multimodal AI, while promoting open research practices.

This workshop welcomes researchers and practitioners with basic Python programming experience. Familiarity with Google Colaboratory is recommended. To participate fully, please ensure the following:

- Bring a laptop with Wi-Fi capability
- Have a Google account (https://accounts.google.com/signin) to access and run the tutorials using Google Colab
- Have a GitHub account  (https://github.com/signup) to make contributions and use GitHub Discussions.

We will conduct this workshop via a Jupyter Book at [https://pykale.github.io/mmai-tutorials/](https://pykale.github.io/mmai-tutorials/), with four interactive tutorials.

<!-- The first part of the workshop will introduce open research practices in biomedical multimodal AI. It will begin with an overview of open research in this field, followed by hands-on tutorials covering four practical examples:

- Cardiovascular disease assessment
- Brain disorder diagnosis
- Cancer classification
- Drug–target prediction

These tutorials will use public imaging, omics, and molecular datasets, including MIMIC ([Chest X-ray](https://physionet.org/content/mimic-cxr/2.1.0/) and [ECG](https://physionet.org/content/mimic-iv-ecg/1.0/)), [ABIDE](https://fcon_1000.projects.nitrc.org/indi/abide/abide_I.html), [TCGA](https://www.cancer.gov/ccg/research/genome-sequencing/tcga), [BindingDB](https://www.bindingdb.org/rwd/bind/index.jsp), and [BioSNAP](https://snap.stanford.edu/biodata/), and follow a standardised machine learning pipeline: data loading, preprocessing, embedding, prediction, evaluation, and interpretation, using the open-source multimodal AI library [PyKale](https://github.com/pykale/pykale).

The second part will allow participants to choose one of the four application areas introduced earlier, brain, heart, cancer, or drug, and further explore relevant challenges through collaborative, hands-on activities in small groups. This session is designed to foster deeper engagement, creativity, and problem-solving, supported by the organisers, speakers, and additional demonstrators available both in person and remotely to provide guidance and answer questions throughout. -->
